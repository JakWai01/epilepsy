{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7d930bc",
   "metadata": {},
   "source": [
    "# UCI Epileptic Seizure Recognition Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e22cacb",
   "metadata": {},
   "source": [
    "Link to the dataset: https://archive.ics.uci.edu/ml/datasets/Epileptic+Seizure+Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "id": "4f59d775",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "a290e7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UCIEpilepsy(Dataset):\n",
    "    def __init__(self, data_path):\n",
    "        self.data = pd.read_csv(data_path).iloc[:, 1:].to_numpy().astype(np.single)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):        \n",
    "        data = torch.from_numpy(self.data[idx,2:])\n",
    "        label = torch.from_numpy(self.data[idx,178:])\n",
    "        return data, label\n",
    "\n",
    "dataset = UCIEpilepsy(data_path='data/data.csv')\n",
    "batch_size = 64\n",
    "test_split = .2\n",
    "shuffle_dataset = True\n",
    "random_seed = 42\n",
    "dataset_size = len(dataset)\n",
    "\n",
    "# Creating data indices for training and validation splits:\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(test_split * dataset_size))\n",
    "if shuffle_dataset:\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "test_indices, train_indices = indices[:split], indices[split:]\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "test_sampler = SubsetRandomSampler(test_indices)\n",
    "\n",
    "train_dataloader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "test_dataloader = DataLoader(dataset, batch_size=batch_size, sampler=test_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "c9944958",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(177, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 5),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear_relu_stack(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "32a7706f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=177, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=128, out_features=5, bias=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "5701ee11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "d320689d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2): # loop over the dataset multiple times\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels.squeeze(1).long()-1)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999: # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                 (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "            \n",
    "print('Finished Training')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
